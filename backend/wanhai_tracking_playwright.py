import argparse
import json
import os
import sys
import time
import re
from datetime import datetime

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

def normalize_date_text(s: str) -> str:
                import re
                s = (s or '').strip()
                if not s: return s
                s = re.sub(r"\(.*?\)", "", s)  # remove (Local Time) etc.
                s = s.replace('/', '-').replace(',', ' ').strip()
                m = re.match(r"^(\d{4})-(\d{1,2})-(\d{1,2})", s)
                if m:
                    return f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
                months = {'JAN':'01','FEB':'02','MAR':'03','APR':'04','MAY':'05','JUN':'06','JUL':'07','AUG':'08','SEP':'09','OCT':'10','NOV':'11','DEC':'12'}
                su = s.upper()
                m = re.match(r"^([0-3]?\d)[-\s](JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[-\s](\d{4})$", su)
                if m:
                    return f"{m.group(3)}-{months[m.group(2)]}-{int(m.group(1)):02d}"
                m = re.match(r"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[-\s]([0-3]?\d)[-\s](\d{4})$", su)
                if m:
                    return f"{m.group(3)}-{months[m.group(1)]}-{int(m.group(2)):02d}"
                return s

def log(msg: str) -> None:
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    # å°†æ—¥å¿—å†™åˆ° stderrï¼Œé¿å…å¹²æ‰° stdout çš„ JSON
    print(f"[wanhai] {ts} {msg}", file=sys.stderr, flush=True)


def load_config(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

# --- å·¥å…·ï¼šæ‰“å°é“¾æ¥ä¿¡æ¯ ---
def debug_link_info(p, el):
    try:
        href = p.evaluate("e => e.getAttribute('href')", el)
        onclick = p.evaluate("e => e.getAttribute('onclick')", el)
        tgt  = p.evaluate("e => e.target || ''", el)
        rect = p.evaluate("e => {const r=e.getBoundingClientRect();return {x:r.x,y:r.y,w:r.width,h:r.height,vis:!!(e.offsetParent)}}", el)
        log(f"link info: href={href} onclick={onclick} target={tgt} rect={rect}")
    except Exception as e:
        log(f"link info failed: {e}")
def click_detail_link(curr_page, link_locator, ctx):
    el = link_locator.first
    try:
        link_locator.first.scroll_into_view_if_needed(timeout=2000)
    except Exception:
        pass
    debug_link_info(curr_page, el)

    # 1) åŒé¡µå¯¼èˆªä¼˜å…ˆ
    try:
        curr_page.evaluate("e => e.target = '_self'", el)
    except Exception:
        pass
    try:
        with curr_page.expect_navigation(timeout=12000):
            el.click()
        log(f"same-page navigation to {curr_page.url}")
        return curr_page
    except Exception as e:
        log(f"no same-page nav: {e}")

    # 2) å¼¹çª—
    try:
        with curr_page.expect_popup(timeout=8000) as pinfo:
            el.click()
        new_pg = pinfo.value
        new_pg.wait_for_load_state("domcontentloaded", timeout=15000)
        log(f"popup opened: {new_pg.url}")
        return new_pg
    except Exception as e:
        log(f"no popup: {e}")

    # 3) ç›´æ¥æ‰§è¡Œ onclick / è·Ÿéš href
    try:
        onclick = curr_page.evaluate("e => e.getAttribute('onclick')", el)
        if onclick:
            log(f"eval onclick: {onclick[:120]}")
            curr_page.evaluate("(el)=>{el.target='_self'; el.click();}", el)
            try:
                curr_page.wait_for_load_state("domcontentloaded", timeout=12000)
            except Exception:
                pass
            return curr_page
    except Exception as e:
        log(f"onclick eval failed: {e}")

    try:
        href = curr_page.evaluate("e => e.getAttribute('href')", el)
        if href and href != '#':
            abs_url = curr_page.evaluate("(u)=>new URL(u, location.href).toString()", href)
            log(f"goto href: {abs_url}")
            curr_page.goto(abs_url, wait_until="domcontentloaded")
            return curr_page
    except Exception as e:
        log(f"goto href failed: {e}")

    log("click attempts exhausted for this link")
    return None
def scrape(config: dict) -> dict:
    
    search_url = config.get("search_url")
    search_input_xpath = config.get("search_input_xpath")
    search_button_xpath = config.get("search_button_xpath")
    more_details_button_xpath = config.get("more_details_button_xpath")
    list_bl_data_xpath = config.get("list_bl_data_xpath")  # åˆ—è¡¨é¡µä¸­çš„ B/L Data æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
    result_xpath = config.get("result_xpath")
    search_number = config.get("search_number")
    headless = bool(config.get("headless", True))
    manual_verify = bool(config.get("manual_verify", False))
    user_data_dir = config.get("user_data_dir")

    if not search_url or not search_input_xpath or not search_button_xpath or not result_xpath:
        raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µï¼šsearch_url / search_input_xpath / search_button_xpath / result_xpath")
    if not more_details_button_xpath:
        raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µï¼šmore_details_button_xpath")

    if not user_data_dir:
        # fallback åˆ°é¡¹ç›®å†…ç›®å½•
        user_data_dir = os.path.join(os.path.dirname(__file__), "app", "userdata")
    # ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºç‹¬ç«‹ä¼šè¯ç›®å½•ï¼Œé¿å…å¹¶å‘äº’ç›¸é”å®š/å¹²æ‰°
    provider_root = os.path.join(user_data_dir, "wanhai")
    session_dir = os.path.join(provider_root, f"session_{int(time.time()*1000)}_{os.getpid()}")
    ensure_dir(session_dir)

    # è°ƒè¯•ç›®å½•ï¼ˆç”¨äºæˆªå›¾ä¸ HARï¼‰
    debug_dir = os.path.join(os.path.dirname(__file__), "app", "debug")
    ensure_dir(debug_dir)

    # ä»…ä¿ç•™ after_open_detail çš„å¤šé¡µé¢æˆªå›¾
    only_detail_snaps = True

    # é€šç”¨æˆªå›¾å·¥å…·ï¼šä¸ºæ¯ä¸€æ­¥ä¿å­˜å¸¦åºå·å’Œç¼–å·çš„æˆªå›¾
    screenshot_seq = 0
    def _san_label(s: str) -> str:
        try:
            return re.sub(r"[^a-zA-Z0-9._-]+", "_", str(s))[:80]
        except Exception:
            return "snap"
    def snap(page, label: str):
        nonlocal screenshot_seq
        if only_detail_snaps and not str(label).startswith("all_after_open_detail__"):
            return None
        try:
            screenshot_seq += 1
            fname = f"wanhai_{str(search_number)}_{screenshot_seq:03d}_{_san_label(label)}.png"
            out_path = os.path.join(debug_dir, fname)
            page.screenshot(path=out_path, full_page=True)
            return out_path
        except Exception:
            return None

    log(f"launch chromium persistent context, headless={headless}, user_data_dir={session_dir}")
    with sync_playwright() as p:
        context = p.chromium.launch_persistent_context(
            user_data_dir=session_dir,
            headless=headless,
            viewport={"width": 1280, "height": 900},
            record_har_path=os.path.join(debug_dir, "wanhai.har")
        )
        # æˆªå–å½“å‰ä¸Šä¸‹æ–‡å†…æ‰€æœ‰å·²æ‰“å¼€é¡µé¢çš„å·¥å…·
        def snap_all_pages(label: str) -> None:
            try:
                for idx, pg in enumerate(context.pages):
                    try:
                        url_ok = getattr(pg, "url", None)
                        if url_ok and url_ok != "about:blank":
                            snap(pg, f"all_{label}__{idx}")
                    except Exception:
                        continue
            except Exception:
                pass
        # ç›‘å¬æ–°é¡µé¢å‡ºç°ï¼Œè‡ªåŠ¨æˆªå›¾ï¼ˆç¦ç”¨ï¼Œé¿å…äº§ç”Ÿé after_open_detail çš„æˆªå›¾ï¼‰
        try:
            def _on_new_page(pg):
                return
            context.on("page", _on_new_page)
        except Exception:
            pass
        try:
            context.tracing.start(screenshots=True, snapshots=True, sources=True)
            log("tracing started")
        except Exception as e:
            log(f"tracing start failed: {e}")
        
        # ç­‰å¾…é¡µé¢ä»åŠ è½½æ€æ¢å¤ä¸ºç¨³å®šæ€çš„å·¥å…·ï¼ˆè§£å†³æˆªå›¾æ—¶ä»åœ¨è½¬åœˆçš„é—®é¢˜ï¼‰
        def _is_loader_visible(pg) -> bool:
            try:
                res = pg.evaluate(
                    """
                    () => {
                        const visible = el => el && !!(el.offsetParent);
                        const sels = [
                          '.ui-widget-overlay', '.ui-blockui', '.ui-blockui-content', '.blockUI',
                          '.loading', '.spinner', '.fa-spinner', 'img[src*="loading"]'
                        ];
                        for (const s of sels) {
                          const el = document.querySelector(s);
                          if (visible(el)) return true;
                        }
                        return false;
                    }
                    """
                )
                return bool(res)
            except Exception:
                return False

        def wait_page_stable(pg, max_wait_sec: int = 12):
            deadline = time.time() + max_wait_sec
            last_len = -1
            stable_hits = 0
            while time.time() < deadline:
                try:
                    pg.wait_for_load_state('domcontentloaded', timeout=1500)
                except Exception:
                    pass
                try:
                    pg.wait_for_load_state('networkidle', timeout=1200)
                except Exception:
                    pass
                try:
                    cur_len = pg.evaluate("() => (document.body && document.body.innerText || '').length")
                except Exception:
                    cur_len = -1
                has_loader = _is_loader_visible(pg)
                if not has_loader and cur_len == last_len and cur_len > 0:
                    stable_hits += 1
                    if stable_hits >= 2:
                        break
                else:
                    stable_hits = 0
                last_len = cur_len
                time.sleep(0.4)
        try:
            context.set_default_timeout(15000)
            page = context.new_page()
            page.set_default_timeout(15000)
            try:
                page.on("dialog", lambda d: (log(f"dialog: {d.message}"), d.accept()))
                page.on("console", lambda m: log(f"console[{m.type}] {m.text}"))
                page.on("pageerror", lambda e: log(f"pageerror: {e}"))
            except Exception:
                pass

            start_ts = time.time()
            def elapsed_ms() -> int:
                try:
                    return int((time.time() - start_ts) * 1000)
                except Exception:
                    return -1
            def expired(limit: int = 90) -> bool:
                return (time.time() - start_ts) > limit
            log(f"goto: {search_url}")
            page.goto(search_url, wait_until="domcontentloaded")
            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_goto.png
            snap(page, "after_goto")

            # ç­‰å¾…ä¸è¾“å…¥
            page.locator(f"xpath={search_input_xpath}").wait_for(timeout=15000)
            page.locator(f"xpath={search_input_xpath}").fill(str(search_number))
            log(f"filled search number: {search_number}")
            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_fill.png
            snap(page, "after_fill")

            # ç‚¹å‡»å‰è®°å½•é¡µé¢ä¿¡æ¯
            try:
                page_title_before = page.title()
            except Exception:
                page_title_before = ""
            url_before = page.url
            log(f"before search click: url={url_before} title={page_title_before}")

            # ç¬¬ä¸€æ¬¡ç‚¹å‡»ï¼šæŸ¥è¯¢æŒ‰é’®ï¼Œå¯èƒ½æ–°å¼€çª—å£æˆ–å½“å‰é¡µè·³è½¬
            search_btn = page.locator(f"xpath={search_button_xpath}")
            vis = search_btn.is_visible()
            en = search_btn.is_enabled()
            log(f"search button state: visible={vis} enabled={en}")
            log("click search button ...")
            new_page = None
            clicked_search_ok = False
            # ä¼˜å…ˆæ•è·æ–°é¡µ
            try:
                with context.expect_page(timeout=8000) as pinfo:
                    search_btn.click()
                new_page = pinfo.value
                new_page.wait_for_load_state("domcontentloaded", timeout=30000)
                clicked_search_ok = True
                log("new page opened after search click")
                snap(new_page, "popup_after_search")
            except Exception as e:
                log(f"no new page after search click: {e}")
                # é€€å›æœ¬é¡µå¯¼èˆª
                try:
                    with page.expect_navigation(timeout=15000):
                        search_btn.click()
                    clicked_search_ok = True
                    log("navigated on same page after search click")
                    snap(page, "after_search_nav")
                except Exception as e2:
                    log(f"no navigation after search click, fallback no_wait_after: {e2}")
                    try:
                        search_btn.click(no_wait_after=True)
                        page.wait_for_load_state("domcontentloaded", timeout=15000)
                        clicked_search_ok = True
                        snap(page, "after_search_nowait")
                    except Exception as e3:
                        log(f"search click failed: {e3}")
            current_page = new_page or page
            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_search_click.png
            snap_all_pages("after_search_click")

            # ç‚¹å‡»åè®°å½•é¡µé¢ä¿¡æ¯
            try:
                cur_title = current_page.title()
            except Exception:
                cur_title = ""
            log(f"after search click: new_page={bool(new_page)} url={current_page.url} title={cur_title} clicked_ok={clicked_search_ok}")

            # ç¬¬äºŒæ¬¡ç‚¹å‡»ï¼šæ›´å¤šè¯¦æƒ…ï¼ˆä¼˜å…ˆç‚¹å‡» "B/L Data"/"Booking Data"ï¼‰ï¼ŒåŒæ ·å¯èƒ½æ–°å¼€çª—å£æˆ–å½“å‰é¡µè·³è½¬
            final_page = None
            clicked_more_ok = False
            # å¤šç­–ç•¥æŸ¥æ‰¾ detail é“¾æ¥
            link_clicked = False
            # === è°ƒè¯•ï¼šæ‰“å°å½“å‰é¡µæ‰€æœ‰å¯è§å« Data/Detail çš„å…ƒç´ ï¼Œæ–¹ä¾¿çœ‹ä¸ºä»€ä¹ˆç‚¹ä¸åˆ° ===
            try:
                names = page.evaluate("""
                    () => Array.from(document.querySelectorAll('a,button'))
                        .filter(el => el.offsetParent !== null)
                        .map(el => (el.innerText||'').trim())
                        .filter(t => /data|detail/i.test(t))
                        .slice(0,50)
                """)
                log(f"visible action texts: {names}")
            except Exception:
                pass
            # === è°ƒè¯•ç»“æŸ ===
            # è·Ÿè¸ªè¿™æ¬¡ç‚¹å‡»å¯¹åº”çš„ ref_typeï¼ˆB/L=MFTï¼ŒBooking=BKGï¼‰
            ref_type_detected = "MFT"
            clicked_kind = None
            try:
                patterns = [(r"B\s*/?\s*L\s*Data", "MFT"),
                            (r"Booking\s*Data",     "BKG")]
                for pat, tag in patterns:
                    try:
                        link = current_page.get_by_role("link", name=re.compile(pat, re.I))
                        cnt = link.count()
                        log(f"detail link '{pat}' count={cnt}")
                        if cnt > 0:
                            ref_type_detected = tag
                            clicked_kind = pat
                            try:
                                link.first.scroll_into_view_if_needed(timeout=2000)
                            except Exception:
                                pass
                            # ä¾æ¬¡å°è¯•ï¼špopup â†’ navigation â†’ href goto
                            el = link.first
                            try:
                                with current_page.expect_popup(timeout=8000) as ppop:
                                    el.click()
                                final_page = ppop.value
                                final_page.wait_for_load_state("domcontentloaded", timeout=30000)
                                clicked_more_ok = True
                                link_clicked = True
                                log("opened popup after detail link click")
                                snap(final_page, "popup_after_detail")
                            except Exception as e_pop:
                                log(f"no popup on detail link: {e_pop}")
                                if not link_clicked:
                                    try:
                                        try:
                                            current_page.evaluate("el => el.target='_self'", el)
                                        except Exception:
                                            pass
                                        with current_page.expect_navigation(timeout=15000):
                                            el.click()
                                        clicked_more_ok = True
                                        link_clicked = True
                                        log("navigated on same page after detail link click")
                                        snap(current_page, "after_detail_nav")
                                    except Exception as e_nav:
                                        log(f"no navigation after detail link: {e_nav}")
                            if not link_clicked:
                                try:
                                    href = current_page.evaluate("el => el.href || el.getAttribute('href')", el)
                                    if href and href != '#':
                                        current_page.goto(href, wait_until="domcontentloaded")
                                        clicked_more_ok = True
                                        link_clicked = True
                                        log(f"goto href after detail link: {href}")
                                        snap(current_page, "after_detail_goto")
                                except Exception as e_goto:
                                    log(f"goto href failed: {e_goto}")
                            break
                    except Exception:
                        continue
            except Exception:
                pass

            if not link_clicked:
                # é€€å› XPath
                try:
                    log("wait more-details button (xpath) ...")
                    current_page.locator(f"xpath={more_details_button_xpath}").wait_for(timeout=15000)
                    more_btn = current_page.locator(f"xpath={more_details_button_xpath}")
                    m_vis = more_btn.is_visible()
                    m_en = more_btn.is_enabled()
                    log(f"more-details button state: visible={m_vis} enabled={m_en}")
                    # æ•æ‰ popup â†’ æœ¬é¡µå¯¼èˆª â†’ href goto
                    el2 = more_btn.first
                    try:
                        with current_page.expect_popup(timeout=8000) as ppop2:
                            el2.click()
                        final_page = ppop2.value
                        final_page.wait_for_load_state("domcontentloaded", timeout=30000)
                        clicked_more_ok = True
                        log("opened popup after more-details click")
                        snap(final_page, "popup_after_more_details")
                    except Exception as e:
                        log(f"no popup after more-details click: {e}")
                        try:
                            try:
                                current_page.evaluate("el => el.target='_self'", el2)
                            except Exception:
                                pass
                            with current_page.expect_navigation(timeout=15000):
                                el2.click()
                            clicked_more_ok = True
                            log("navigated on same page after more-details click")
                            snap(current_page, "after_more_details_nav")
                        except Exception as e2:
                            log(f"no navigation after more-details, try goto href: {e2}")
                            try:
                                href2 = current_page.evaluate("el => el.href || el.getAttribute('href')", el2)
                                if href2 and href2 != '#':
                                    current_page.goto(href2, wait_until="domcontentloaded")
                                    clicked_more_ok = True
                                    log(f"goto href after more-details: {href2}")
                                    snap(current_page, "after_more_details_goto")
                            except Exception as e3:
                                log(f"goto href failed: {e3}")
                except Exception as e:
                    log(f"xpath more-details click error: {e}")
                    # æœ€åå…œåº•ï¼šJS æŸ¥æ‰¾åŒ…å«æ–‡æœ¬çš„é“¾æ¥
                    try:
                        current_page.evaluate("""
                            () => {
                              const as = Array.from(document.querySelectorAll('a'));
                              const target = as.find(a => /B\s*\/?.?\s*L\s*Data|Booking\s*Data/i.test(a.textContent||''));
                              if (target) { target.click(); return true; }
                              return false;
                            }
                        """)
                        clicked_more_ok = True
                    except Exception:
                        pass

            # å¦‚æœæ–°å¼€äº†é¡µï¼Œåˆ‡æ¢ï¼›å¦åˆ™æ²¿ç”¨å½“å‰é¡µ
            try:
                candidate = next((pg for pg in context.pages if pg is not current_page and pg.url != "about:blank"), None)
                if candidate:
                    final_page = candidate
                    snap(candidate, "candidate_selected")
            except Exception:
                pass
            last_page = final_page or current_page
            try:
                last_page.wait_for_load_state("domcontentloaded", timeout=15000)
            except Exception:
                pass
            snap(last_page, "after_last_page_ready")
            # å¦‚æœå½“å‰æ˜¯åˆ—è¡¨é¡µï¼ˆtracking_data_listï¼‰ï¼Œç»§ç»­ç‚¹å‡» "B/L Data" è¿›å…¥è¯¦æƒ…é¡µ
            # å¦‚æœå½“å‰æ˜¯åˆ—è¡¨é¡µï¼ˆtracking_data_listï¼‰ï¼Œç»§ç»­ç‚¹å‡» "B/L Data" è¿›å…¥è¯¦æƒ…é¡µ
            # ========= PATCH: å¼ºåŒ–ç‰ˆ - æ‰“å¼€ B/L Data è¯¦æƒ…é¡µ =========
            try:
                if re.search(r"tracking_data_list", (last_page.url or ""), re.I):
                    log("list page detected, trying to open 'B/L Data' detail page ...")

                    clicked = False
                    found_detail = None

                    # Step 1ï¸âƒ£: å°è¯•ç›´æ¥è°ƒç”¨ JS å‡½æ•° formblSubmit(ref_no,'MFT')
                    try:
                        ok_eval = last_page.evaluate("""
                            (refNo, rt) => {
                                try {
                                    if (typeof window.formblSubmit === 'function') {
                                        window.formblSubmit(refNo, rt);
                                        return true;
                                    }
                                    return false;
                                } catch(e) { return false; }
                            }
                        """, str(search_number), ref_type_detected)
                        if ok_eval:
                            clicked = True
                            log(f"invoked window.formblSubmit(ref_no,'{ref_type_detected}')")

                    except Exception as e:
                        log(f"formblSubmit invoke error: {e}")

                    # Step 2ï¸âƒ£: å¦‚æœé¡µé¢å‡½æ•°æ²¡æ‰§è¡Œï¼Œå°±å°è¯•ç‚¹å‡»â€œB/L Dataâ€é“¾æ¥
                    if not clicked:
                        try:
                            if list_bl_data_xpath:
                                bl_btn = last_page.locator(f"xpath={list_bl_data_xpath}")
                                if bl_btn.count() > 0:
                                    bl_btn.first.scroll_into_view_if_needed(timeout=2000)
                                    with last_page.context.expect_page(timeout=8000) as ppop:
                                        bl_btn.first.click()
                                    found_detail = ppop.value
                                    clicked = True
                                    log("clicked list B/L via config xpath -> popup appeared")
                            if not clicked:
                                xp = "//a[contains(normalize-space(.),'B/L Data')]"
                                l = last_page.locator(f"xpath={xp}")
                                if l.count() > 0:
                                    l.first.scroll_into_view_if_needed(timeout=2000)
                                    with last_page.context.expect_page(timeout=8000) as ppop2:
                                        l.first.click()
                                    found_detail = ppop2.value
                                    clicked = True
                                    log("clicked list B/L via fallback xpath -> popup appeared")
                        except Exception as e:
                            log(f"click B/L link error: {e}")

                    # Step 3ï¸âƒ£: ç­‰å¾… JS æ‰“å¼€çš„å¼¹çª—
                    if clicked and not found_detail:
                        log("waiting for popup detail window (including JS-opened) ...")
                        target_url_part = "tracking_data_page_by_bl_redirect"
                        for _ in range(12):  # æœ€å¤š12ç§’
                            for p in context.pages:
                                if target_url_part in (p.url or ""):
                                    found_detail = p
                                    break
                            if found_detail:
                                break
                            time.sleep(1)

                    # Step 4ï¸âƒ£: å¦‚æœä»æœªæ£€æµ‹åˆ°å¼¹çª—ï¼Œå¼ºåˆ¶æ„é€  URL è·³è½¬
                    # Step 4ï¸âƒ£: å¦‚æœä»æœªæ£€æµ‹åˆ°å¼¹çª—ï¼Œå¼ºåˆ¶æ„é€  URL è·³è½¬
                    if not found_detail:
                        try:
                            # ç¬¬ä¸€å±‚ï¼šæ—§çš„ä¸­è½¬é¡µï¼ˆå¯èƒ½ç«‹å³302è·³å›åˆ—è¡¨ï¼‰
                            redirect_page = ("tracking_data_page_by_bl_redirect"
                                             if ref_type_detected == "MFT"
                                             else "tracking_data_page_by_booking_redirect")
                            forced_url = (
                                f"https://www.wanhai.com/views/cargo_track_v2/"
                                f"{redirect_page}.xhtml?ref_no={search_number}&ref_type={ref_type_detected}"
                            )
                            log(f"force goto detail page: {forced_url}")
                            last_page.goto(forced_url, wait_until="domcontentloaded", timeout=20000)
                            log("force goto success (detail page in same tab)")
                            snap(last_page, "after_force_redirect")

                            # âš ï¸ ç¬¬äºŒå±‚ï¼šç›´æ¥è·³çœŸæ­£çš„ç»“æœé¡µï¼ˆç»•è¿‡redirectï¼‰
                            real_detail_url = (
                                f"https://www.wanhai.com/views/cargo_track_v2/"
                                f"tracking_data_page.xhtml?ref_no={search_number}&ref_type={ref_type_detected}"
                            )
                            log(f"manual goto REAL detail page: {real_detail_url}")
                            last_page.goto(real_detail_url, wait_until="domcontentloaded", timeout=25000)
                            log("navigated to REAL detail page successfully")
                            snap(last_page, "after_force_real_detail")

                        except Exception as e:
                            log(f"force goto REAL detail page failed: {e}")

                    # Step 5ï¸âƒ£: è°ƒè¯•æˆªå›¾
                    # ä¿ç•™ after_open_detail çš„å¤šé¡µé¢å¿«ç…§ï¼Œç”± snap_all_pages ç»Ÿä¸€ç”Ÿæˆ

            except Exception as e:
                log(f"list B/L open section failed: {e}")
            # ========= PATCH END =========


            # ä¸å†ä¾èµ– URL å˜åŒ–ï¼›Wan Hai å¸¸ä¸º JSF åŒé¡µå±€éƒ¨æ›´æ–°
            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_more_details.png

            try:
                last_title = last_page.title()
            except Exception:
                last_title = ""
            # è®°å½• frame ä¿¡æ¯ï¼Œä¾¿äºæ’æŸ¥
            try:
                frs = getattr(last_page, "frames", [])
                log(f"final page frames: count={len(frs)} urls={[getattr(f, 'url', None) for f in frs][:5]}")
            except Exception:
                pass
            log(f"after more-details click: new_page={bool(final_page)} url={last_page.url} title={last_title} clicked_ok={clicked_more_ok}")

            # URL å®ˆå«ï¼šè‹¥è¯¯è½å›æŸ¥è¯¢é¡µï¼Œåˆ™å¼ºåˆ¶èµ°åŒé¡µå¯¼èˆªé‡è¯•ä¸€æ¬¡ï¼ˆå…ˆ B/L Data å† Booking Dataï¼‰
            
            # ========= FINAL PATCH: ç¨³å®šç‰ˆ æ‰“å¼€ B/L Data å¹¶å¼ºåˆ¶è¿›å…¥çœŸå®é¡µé¢ =========
            try:
                if re.search(r"tracking_data_list", (last_page.url or ""), re.I):
                    log("list page detected, trying to open 'B/L Data' detail page ...")

                    clicked = False
                    found_detail = None

                    # Step 1ï¸âƒ£: è°ƒç”¨ JS å‡½æ•° formblSubmit(ref_no,'MFT')
                    try:
                        ok_eval = last_page.evaluate("""
                            (refNo, rt) => {
                                try {
                                    if (typeof window.formblSubmit === 'function') {
                                        window.formblSubmit(refNo, rt);
                                        return true;
                                    }
                                    return false;
                                } catch(e) { return false; }
                            }
                        """, str(search_number), ref_type_detected)
                        if ok_eval:
                            clicked = True
                            log(f"invoked window.formblSubmit(ref_no,'{ref_type_detected}')")

                    except Exception as e:
                        log(f"formblSubmit invoke error: {e}")

                    # Step 2ï¸âƒ£: å°è¯•ç‚¹å‡» "B/L Data" é“¾æ¥
                    if not clicked:
                        try:
                            xp = "//a[contains(normalize-space(.),'B/L Data')]"
                            l = last_page.locator(f"xpath={xp}")
                            if l.count() > 0:
                                l.first.scroll_into_view_if_needed(timeout=2000)
                                with last_page.context.expect_page(timeout=8000) as ppop:
                                    l.first.click()
                                found_detail = ppop.value
                                clicked = True
                                log("clicked list B/L via fallback xpath -> popup appeared")
                                snap(found_detail, "detail_popup_v2")
                        except Exception as e:
                            log(f"click B/L link error: {e}")

                    # Step 3ï¸âƒ£: ç­‰å¾… window.open å¼¹çª—
                    if clicked and not found_detail:
                        log("waiting for popup detail window (including JS-opened) ...")
                        target_url_part = "tracking_data_page_by_bl_redirect"
                        for _ in range(12):
                            for p in context.pages:
                                if target_url_part in (p.url or ""):
                                    found_detail = p
                                    break
                            if found_detail:
                                break
                            time.sleep(1)

                    # Step 4ï¸âƒ£: å¦‚æœä»æœªæ£€æµ‹åˆ°å¼¹çª— -> å¼ºåˆ¶è¿›å…¥çœŸå®é¡µé¢
                    if not found_detail:
                        try:
                            redirect_page = ("tracking_data_page_by_bl_redirect"
                                             if ref_type_detected == "MFT"
                                             else "tracking_data_page_by_booking_redirect")
                            forced_url = (
                                f"https://www.wanhai.com/views/cargo_track_v2/"
                                f"{redirect_page}.xhtml?ref_no={search_number}&ref_type={ref_type_detected}"
                            )
                            log(f"force goto redirect page: {forced_url}")
                            last_page.goto(forced_url, wait_until="domcontentloaded", timeout=20000)
                            log("force goto redirect success")
                            snap(last_page, "after_force_redirect_v2")

                            # ç›´æ¥è¿›å…¥çœŸå®é¡µé¢ï¼ˆç»•è¿‡redirectï¼‰
                            real_detail_url = (
                                f"https://www.wanhai.com/views/cargo_track_v2/"
                                f"tracking_data_page.xhtml?ref_no={search_number}&ref_type={ref_type_detected}"
                            )
                            log(f"manual goto REAL detail page: {real_detail_url}")
                            last_page.goto(real_detail_url, wait_until="domcontentloaded", timeout=25000)
                            log("navigated to REAL detail page successfully")
                            snap(last_page, "after_force_real_detail_v2")

                            # ğŸ” è°ƒè¯•æˆªå›¾
                            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_real_detail.png

                        except Exception as e:
                            log(f"force goto REAL detail page failed: {e}")
                    else:
                        log(f"found new detail page: {found_detail.url}")
                        last_page = found_detail
                        try:
                            last_page.wait_for_load_state("domcontentloaded", timeout=15000)
                        except Exception:
                            pass

                    # Step 5ï¸âƒ£: è°ƒè¯•æˆªå›¾
                    # ä¿ç•™ after_open_detail çš„å¤šé¡µé¢å¿«ç…§ï¼Œç”± snap_all_pages ç»Ÿä¸€ç”Ÿæˆ

            except Exception as e:
                log(f"list B/L open section failed: {e}")
            # ========= FINAL PATCH END =========

            # âœ… fallbackï¼šè‹¥ ETA æœªå‡ºç°åœ¨åˆ—è¡¨é¡µï¼Œç«‹å³å¼ºåˆ¶è¿›å…¥çœŸå® detail é¡µé¢
            try:
                if re.search(r"tracking_data_list", (last_page.url or ""), re.I):
                    def extract_eta_from_list(page):
                        try:
                            page.wait_for_selector("table.ui-datatable, .ui-datatable-tablewrapper table", timeout=8000)
                        except Exception:
                            return '', 'datatable not found'

                        js = """
                        () => {
                        const norm = s => (s||'').replace(/\u00A0/g,' ').replace(/\s+/g,' ').trim().toUpperCase();
                        const wrap = document.querySelector('.ui-datatable-tablewrapper') || document;
                        const table = wrap.querySelector('table');
                        if (!table) return ['', 'no table'];
                        const ths = Array.from(table.querySelectorAll('thead th'));
                        const isETA = (t) => {
                            const x = norm(t);
                            return x === 'ETA' || (x.includes('EST') && x.includes('ARRIVAL'));
                        };
                        let etaIdx = -1;
                        ths.forEach((th,i)=>{ if (etaIdx<0 && isETA(th.textContent||'')) etaIdx = i; });
                        if (etaIdx < 0) return ['', 'eta header not found'];
                        const rows = Array.from(table.querySelectorAll('tbody tr')).filter(r => r.offsetParent !== null);
                        if (!rows.length) return ['', 'no rows'];
                        const tds = Array.from(rows[0].querySelectorAll('td'));
                        if (!tds.length || etaIdx >= tds.length) return ['', 'eta td out of range'];
                        const val = (tds[etaIdx].textContent||'').replace(/\u00A0/g,' ').replace(/\s+/g,' ').trim();
                        return [val, 'ok'];
                        }
                        """
                        try:
                            eta_text, reason = page.evaluate(js)
                            if eta_text:
                                return eta_text, 'ok'
                            return '', reason
                        except Exception as e:
                            return '', f'eval error: {e}'

                    eta_from_list, why = extract_eta_from_list(last_page)
                    if not eta_from_list:
                        log(f"list page ETA not found: {why}; fallback to detail/poller strategy")
                        try:
                            real_detail_url = (
                                f"https://www.wanhai.com/views/cargo_track_v2/"
                                f"tracking_data_page.xhtml?ref_no={search_number}&ref_type={ref_type_detected}"
                            )
                            log(f"fallback: manually goto REAL detail page: {real_detail_url}")
                            last_page.goto(real_detail_url, wait_until="domcontentloaded", timeout=25000)
                            log("fallback: navigated to REAL detail page successfully")
                            # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_real_detail_fallback.png
                        except Exception as e:
                            log(f"fallback: manual goto REAL detail failed: {e}")
            except Exception as e:
                log(f"list ETA extraction failed: {e}")
            # ========= FINAL PATCH END =========

            # åˆ—è¡¨é¡µä¼˜å…ˆï¼šè‹¥å½“å‰ä¸º tracking_data_list.xhtmlï¼Œå…ˆå°è¯•ç›´æ¥ä»åˆ—è¡¨è¡¨æ ¼æå– ETA
            try:
                if re.search(r"tracking_data_list", (last_page.url or ""), re.I):
                    def extract_eta_from_list(page):
                        # è¿”å› (eta_text, debug)ï¼›æ‰¾ä¸åˆ°è¿”å› ('', why)
                        try:
                            page.wait_for_selector("table.ui-datatable, .ui-datatable-tablewrapper table", timeout=8000)
                        except Exception:
                            return '', 'datatable not found'

                        js = """
                        () => {
                          const norm = s => (s||'').replace(/\u00A0/g,' ').replace(/\s+/g,' ').trim().toUpperCase();
                          const wrap = document.querySelector('.ui-datatable-tablewrapper') || document;
                          const table = wrap.querySelector('table');
                          if (!table) return ['', 'no table'];
                          const ths = Array.from(table.querySelectorAll('thead th'));
                          if (!ths.length) return ['', 'no thead'];
                          // æ‰¾ ETA åˆ—ï¼Œå…¼å®¹ "ETA", "EST ARRIVAL", "EST. ARRIVAL", "ESTIMATED ARRIVAL"
                          const isETA = (t) => {
                            const x = norm(t);
                            return x === 'ETA' || (x.includes('EST') && x.includes('ARRIVAL'));
                          };
                          let etaIdx = -1;
                          ths.forEach((th,i)=>{ if (etaIdx<0 && isETA(th.textContent||'')) etaIdx = i; });
                          if (etaIdx < 0) return ['', 'eta header not found'];

                          const rows = Array.from(table.querySelectorAll('tbody tr')).filter(r => r.offsetParent !== null);
                          if (!rows.length) return ['', 'no rows'];
                          // å–ç¬¬ä¸€æ¡æˆ–åŒ…å« â€œB/L Noâ€ åŒ¹é…çš„è¡Œï¼Œè¿™é‡Œå…ˆæ‹¿ç¬¬ä¸€æ¡
                          const tds = Array.from(rows[0].querySelectorAll('td'));
                          if (!tds.length || etaIdx >= tds.length) return ['', 'eta td out of range'];
                          const val = (tds[etaIdx].textContent||'').replace(/\u00A0/g,' ').replace(/\s+/g,' ').trim();
                          return [val, 'ok'];
                        }
                        """
                        try:
                            eta_text, reason = page.evaluate(js)
                            if eta_text:
                                return eta_text, 'ok'
                            return '', reason
                        except Exception as e:
                            return '', f'eval error: {e}'

                    eta_from_list, why = extract_eta_from_list(last_page)
                    if eta_from_list:
                        # è§„èŒƒåŒ–å¹¶ç›´æ¥è¿”å›
                        def _norm_list_date(s: str) -> str:
                            s = (s or '').strip()
                            if not s:
                                return s
                            s = re.sub(r"\(.*?\)", "", s)
                            s = s.replace('/', '-').replace(',', ' ').strip()
                            m = re.match(r"^(\d{4})-(\d{1,2})-(\d{1,2})", s)
                            if m:
                                return f"{m.group(1)}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
                            months = {'JAN':'01','FEB':'02','MAR':'03','APR':'04','MAY':'05','JUN':'06','JUL':'07','AUG':'08','SEP':'09','OCT':'10','NOV':'11','DEC':'12'}
                            su = s.upper()
                            m = re.match(r"^([0-3]?\d)[-\s](JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[-\s](\d{4})$", su)
                            if m:
                                return f"{m.group(3)}-{months[m.group(2)]}-{int(m.group(1)):02d}"
                            m = re.match(r"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)[-\s]([0-3]?\d)[-\s](\d{4})$", su)
                            if m:
                                return f"{m.group(3)}-{months[m.group(1)]}-{int(m.group(2)):02d}"
                            return s

                        before_norm = eta_from_list
                        eta_norm = _norm_list_date(eta_from_list)
                        log(f"ETA from list: before='{before_norm}' after='{eta_norm}'")
                        out_obj = {
                            "status": "ok",
                            "number": str(search_number),
                            "result": eta_norm,
                            "clickedSearch": clicked_search_ok,
                            "clickedMoreDetails": clicked_more_ok,
                        }
                        print(json.dumps(out_obj, ensure_ascii=False), flush=True)
                        out_file = os.path.join(debug_dir, "wanhai_result.json")
                        with open(out_file, "w", encoding="utf-8") as f:
                            json.dump({"timestamp": datetime.now().isoformat(), **out_obj}, f, ensure_ascii=False, indent=2)
                        log(f"written debug: {out_file}")
                        return out_obj
                    else:
                        log(f"list page ETA not found: {why}; fallback to detail/poller strategy")
            except Exception:
                pass
            # ---- å¼ºåˆ¶æ‰“å¼€è¯¦æƒ…é¡µçš„å·¥å…·å‡½æ•°ï¼ˆæ”¾åœ¨å®ˆå«å‰é¢ï¼‰----
            def open_detail_via_query_form(pg, ref_no: str, ref_type: str):
                try:
                    pg.wait_for_selector("#cargoType", timeout=8000)
                    # å¯¹äº Booking/BL éƒ½é€‰ value=2ï¼ˆBook No. / BL no.ï¼‰
                    try:
                        pg.select_option("#cargoType", "2")
                    except Exception:
                        pg.evaluate("() => document.getElementById('cargoType').value='2'")
                    # å¡«ç¼–å·
                    try:
                        pg.fill("#q_ref_no1", ref_no)
                    except Exception:
                        pg.evaluate("(v)=>{const el=document.getElementById('q_ref_no1'); if(el){el.value=v;}}", ref_no)

                    # Query æŒ‰é’®ä¼š target=_blank -> æ–°å¼€é¡µ
                    try:
                        with pg.context.expect_page(timeout=20000) as pinfo:
                            pg.click("input#Query")
                        np = pinfo.value
                        np.wait_for_load_state("domcontentloaded", timeout=25000)
                        log(f'query form opened new page: {np.url}')
                        return np
                    except Exception as e1:
                        log(f"query click no new page: {e1}")
                        # å…œåº•ï¼šåŒé¡µå¯¼èˆª
                        try:
                            with pg.expect_navigation(timeout=20000):
                                pg.click("input#Query")
                            log("query form navigated on same page")
                            return pg
                        except Exception as e2:
                            log(f"query same-page nav failed: {e2}")
                            # ç»ˆæå…œåº•ï¼šç›´æ¥è°ƒç”¨ mojarra.jsfcljs
                            try:
                                pg.evaluate("""
                                    () => {
                                        const f = document.getElementById('cargoTrackV2Bean');
                                        if (f && window.mojarra && window.mojarra.jsfcljs) {
                                            window.mojarra.jsfcljs(f, {'Query':'Query','skipValidate':'true'}, '');
                                        }
                                    }
                                """)
                                # å†è¯•æŠ“æ–°é¡µ
                                try:
                                    with pg.context.expect_page(timeout=20000) as p2:
                                        pass
                                except Exception:
                                    pass
                                # ç­‰å¾…ä»»æ„å¯è§ç»“æœè¡¨/è¯¦æƒ…
                                pg.wait_for_load_state("domcontentloaded", timeout=20000)
                                return pg
                            except Exception as e3:
                                log(f"mojarra submit failed: {e3}")
                                return None
                except Exception as e:
                    log(f"open_detail_via_query_form error: {e}")
                    return None

            def force_open_detail(pg, ref_no: str, ref_type: str) -> bool:
                base = "https://www.wanhai.com/views/cargo_track_v2"
                try:
                    url = f"{base}/tracking_data_page.xhtml?ref_no={ref_no}&ref_type={ref_type}"
                    log(f"force goto REAL detail: {url}")
                    pg.goto(url, wait_until="domcontentloaded", timeout=25000)
                    return True
                except Exception as e1:
                    log(f"real detail goto failed: {e1}")
                    try:
                        page_name = ("tracking_data_page_by_bl_redirect"
                                     if ref_type == "MFT" else
                                     "tracking_data_page_by_booking_redirect")
                        url2 = f"{base}/{page_name}.xhtml?ref_no={ref_no}&ref_type={ref_type}"
                        log(f"fallback redirect goto: {url2}")
                        pg.goto(url2, wait_until="domcontentloaded", timeout=20000)
                        return True
                    except Exception as e2:
                        log(f"redirect goto failed: {e2}")
                        return False
          
            # ---- å¦‚æœè¿˜åœ¨æŸ¥è¯¢é¡µæˆ–åˆ—è¡¨é¡µï¼Œç«‹åˆ»å¼ºåˆ¶è¿›å…¥çœŸå®è¯¦æƒ…é¡µ ----
                        # ---- ä¼˜å…ˆç­–ç•¥ï¼šå¦‚æœè¿˜åœ¨æŸ¥è¯¢é¡µ -> ç”¨è¡¨å•æäº¤ï¼›åˆ—è¡¨é¡µå†è€ƒè™‘å¼ºåˆ¶è·³ ----
            if re.search(r"tracking_query\.xhtml", (last_page.url or ""), re.I):
                np = open_detail_via_query_form(last_page, str(search_number), ref_type_detected)
                if np:
                    last_page = np
                    try:
                        last_page.wait_for_load_state("domcontentloaded", timeout=15000)
                    except Exception:
                        pass
                    # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_query_submit.png
                else:
                    log("query-form submit failed; fallback to force_open_detail()")
                    ok_force = force_open_detail(last_page, str(search_number), ref_type_detected)
                    if ok_force:
                        try:
                            last_page.wait_for_load_state("domcontentloaded", timeout=15000)
                        except Exception:
                            pass
                        try:
                            last_page.screenshot(path=os.path.join(debug_dir, "wanhai_after_force_detail.png"))
                        except Exception:
                            pass
                    else:
                        log("force_open_detail failed; still on query page")

            elif re.search(r"tracking_data_list\.xhtml", (last_page.url or ""), re.I):
                # åˆ—è¡¨é¡µå¯ç»§ç»­å°è¯•å¼ºåˆ¶è¿›å…¥çœŸå®è¯¦æƒ…ï¼ˆæˆ–åé¢å·²æœ‰åˆ—è¡¨ç‚¹å‡»é€»è¾‘ï¼‰
                ok_force = force_open_detail(last_page, str(search_number), ref_type_detected)
                if ok_force:
                    try:
                        last_page.wait_for_load_state("domcontentloaded", timeout=15000)
                    except Exception:
                        pass
                    # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_force_detail.png
                    # ç¦ç”¨æ—©æœŸæˆªå›¾ï¼šwanhai_after_force_detail.png

                        # ---- é€šè¿‡ tracking_query.xhtml è¡¨å•æäº¤æ‰“å¼€è¯¦æƒ…ï¼ˆé¿å…WAF/JSFæ ¡éªŒï¼‰----
            
            # åœ¨æœ€ç»ˆé¡µé¢ç­‰å¾…ç»“æœå¹¶æå–ï¼ˆè½®è¯¢ï¼Œå…¼å®¹ JSF å±€éƒ¨æ›´æ–° / frames / XHTML å‘½åç©ºé—´ï¼‰
            log("waiting result on final page ...")
            # æˆªå–æ‰€æœ‰é¡µé¢å‰ï¼Œç­‰å¾…é¡µé¢ç¨³å®šï¼Œé¿å…åŠ è½½æ€
            try:
                for pg in context.pages:
                    try:
                        wait_page_stable(pg, max_wait_sec=10)
                    except Exception:
                        pass
            except Exception:
                pass
            # æˆªå–æ‰€æœ‰é¡µé¢ï¼Œå¹¶å¯¹ä»¥ __3 ç»“å°¾çš„é‚£å¼ åš OCR æå–æ–‡å­—
            taken = []
            try:
                for idx, pg in enumerate(context.pages):
                    try:
                        url_ok = getattr(pg, "url", None)
                        if url_ok and url_ok != "about:blank":
                            p = snap(pg, f"all_after_open_detail__{idx}")
                            if p:
                                taken.append((idx, p))
                    except Exception:
                        continue
            except Exception:
                pass
            # é€‰æ‹© idx==3 çš„å›¾ç‰‡ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™é€‰æ‹©æœ€åä¸€å¼ 
            target_path = None
            for idx, p in taken:
                if idx == 3:
                    target_path = p
                    break
            if target_path is None and taken:
                target_path = taken[-1][1]
            # OCR æå–å¹¶ç«‹å³ç»“æŸï¼ˆä¸å†ç»§ç»­ç­‰å¾…/è½®è¯¢ï¼‰
            out_obj_early = None
            ocr_text = None
            if target_path:
                try:
                    import pytesseract
                    from PIL import Image
                    img = Image.open(target_path)
                    # è‹¥ç³»ç»Ÿå·²å®‰è£…ä¸­æ–‡è¯­è¨€åŒ…ï¼Œå¯æ”¹ä¸º eng+chi_sim
                    ocr_text = pytesseract.image_to_string(img, lang="eng")
                    txt_path = os.path.join(debug_dir, "wanhai_ocr_detail.txt")
                    with open(txt_path, "w", encoding="utf-8") as f:
                        f.write(ocr_text)
                    log(f"OCR extracted to: {txt_path}")
                    # ç®€å•æ­£åˆ™ä» OCR ä¸­æå– ETA æ—¥æœŸå¹¶è§„èŒƒåŒ–
                    eta_raw = None
                    try:
                        m = re.search(r"ESTIMATED\s*ARRIVAL\s*DATE[^\n\r]*?(\d{4}[\/-]\d{1,2}[\/-]\d{1,2}|[A-Z]{3}[\-\s]\d{1,2}[\-\s]\d{4}|\d{1,2}[\-\s][A-Z]{3}[\-\s]\d{4})", ocr_text, flags=re.I)
                        if not m:
                            m = re.search(r"ETA[^\n\r]*?(\d{4}[\/-]\d{1,2}[\/-]\d{1,2}|[A-Z]{3}[\-\s]\d{1,2}[\-\s]\d{4}|\d{1,2}[\-\s][A-Z]{3}[\-\s]\d{4})", ocr_text, flags=re.I)
                        if m:
                            eta_raw = m.group(1)
                    except Exception:
                        eta_raw = None
                    if eta_raw:
                        eta_norm = normalize_date_text(eta_raw)
                        out_obj_early = {"status": "ok", "number": str(search_number), "result": eta_norm, "source": "ocr"}
                        # ä¹Ÿå†™å…¥æ–‡ä»¶ä¾¿äºæ ¸å¯¹
                        with open(os.path.join(debug_dir, "wanhai_ocr_eta.txt"), "w", encoding="utf-8") as f:
                            f.write(eta_norm)
                    else:
                        out_obj_early = {"status": "ok", "number": str(search_number), "ocr": True, "result": "", "source": "ocr"}
                except Exception as e:
                    log(f"OCR failed: {e}")
                    out_obj_early = {"status": "ok", "number": str(search_number), "ocr_error": str(e)}

            if out_obj_early is None:
                out_obj_early = {"status": "ok", "number": str(search_number), "note": "screenshot saved, no ocr"}
            # æ‰“å°å¹¶ä¿å­˜ç»“æœï¼Œç„¶åç«‹åˆ»è¿”å›ï¼Œé¿å…ç»§ç»­ç­‰å¾…
            try:
                print(json.dumps(out_obj_early, ensure_ascii=False), flush=True)
                out_file = os.path.join(debug_dir, "wanhai_result.json")
                with open(out_file, "w", encoding="utf-8") as f:
                    json.dump({"timestamp": datetime.now().isoformat(), **out_obj_early}, f, ensure_ascii=False, indent=2)
                log(f"written early result: {out_file}")
            except Exception:
                pass
            return out_obj_early
                        # åœ¨æœ€ç»ˆé¡µé¢ç­‰å¾…ç»“æœå¹¶æå–ï¼ˆè½®è¯¢ï¼Œå…¼å®¹ JSF å±€éƒ¨æ›´æ–° / frames / XHTML å‘½åç©ºé—´ï¼‰
           

            # ---------- BEGIN: å³åˆ»å– ETA çš„è½»é‡å…œåº• ----------
            def get_text_by_xpath(page, xpath: str, timeout=8000) -> str:
                """æ ¹æ®XPathå–æ–‡æœ¬"""
                try:
                    loc = page.locator(f"xpath={xpath}")
                    loc.wait_for(state="visible", timeout=timeout)
                    txt = loc.first.text_content() or ""
                    return re.sub(r"\s+", " ", txt.replace("\u00A0"," ")).strip()
                except Exception:
                    return ""

            # === Step 1: å°è¯•ç›¸å¯¹XPathæŠ“ETA ===
            eta_xpath_label = "(//td[translate(normalize-space(.),'abcdefghijklmnopqrstuvwxyz.','ABCDEFGHIJKLMNOPQRSTUVWXYZ ')='ESTIMATED ARRIVAL DATE']/following-sibling::td[1])[last()]"
            eta_text = get_text_by_xpath(last_page, eta_xpath_label, timeout=6000)

            # å¦‚æœæ‰¾åˆ°äº†å°±ç›´æ¥è¿”å›ç»“æœ
            if eta_text:
                before_norm = eta_text
                eta_text = normalize_date_text(eta_text)
                log(f"ETA (via relative XPath): before='{before_norm}' after='{eta_text}'")

                out_obj = {
                    "status": "ok",
                    "number": str(search_number),
                    "result": eta_text,
                    "clickedSearch": clicked_search_ok,
                    "clickedMoreDetails": clicked_more_ok,
                }
                print(json.dumps(out_obj, ensure_ascii=False), flush=True)
                out_file = os.path.join(debug_dir, "wanhai_result.json")
                with open(out_file, "w", encoding="utf-8") as f:
                    json.dump({"timestamp": datetime.now().isoformat(), **out_obj}, f, ensure_ascii=False, indent=2)
                log(f"written debug: {out_file}")
                return out_obj

            # === Step 2: å¦‚æœè¿˜æ²¡æ‹¿åˆ°ï¼Œå°±å°è¯•ç”¨configä¸­é…ç½®çš„ result_xpath ===
            if result_xpath:
                eta_text = get_text_by_xpath(last_page, result_xpath, timeout=4000)
                if eta_text:
                    before_norm = eta_text
                    eta_text = normalize_date_text(eta_text)
                    log(f"ETA (via config result_xpath): before='{before_norm}' after='{eta_text}'")

                    out_obj = {
                        "status": "ok",
                        "number": str(search_number),
                        "result": eta_text,
                        "clickedSearch": clicked_search_ok,
                        "clickedMoreDetails": clicked_more_ok,
                    }
                    print(json.dumps(out_obj, ensure_ascii=False), flush=True)
                    out_file = os.path.join(debug_dir, "wanhai_result.json")
                    with open(out_file, "w", encoding="utf-8") as f:
                        json.dump({"timestamp": datetime.now().isoformat(), **out_obj}, f, ensure_ascii=False, indent=2)
                    log(f"written debug: {out_file}")
                    return out_obj

            # === Step 3: å†å…œåº•ï¼Œç”¨å…¨æ–‡æ­£åˆ™åŒ¹é… ETA æ—¥æœŸ ===
            try:
                fulltxt = last_page.evaluate("() => document.body ? document.body.innerText : ''") or ""
            except Exception:
                fulltxt = ""
            if fulltxt.strip():
                m = re.search(
                    r"(?:ETA|EST\.?\s*ARRIVAL|ESTIMATED\s*ARRIVAL)[^\d]{0,30}"
                    r"(\d{4}[\/\-\.]\d{1,2}[\/\-\.]\d{1,2}|[A-Z]{3}[\-\s]\d{1,2}[\-\s]\d{4}|\d{1,2}[\-\s][A-Z]{3}[\-\s]\d{4})",
                    fulltxt,
                    flags=re.I
                )
                if m:
                    before_norm = m.group(1)
                    eta_text = normalize_date_text(before_norm)
                    log(f"ETA (via full-text regex): before='{before_norm}' after='{eta_text}'")

                    out_obj = {
                        "status": "ok",
                        "number": str(search_number),
                        "result": eta_text,
                        "clickedSearch": clicked_search_ok,
                        "clickedMoreDetails": clicked_more_ok,
                    }
                    print(json.dumps(out_obj, ensure_ascii=False), flush=True)
                    out_file = os.path.join(debug_dir, "wanhai_result.json")
                    with open(out_file, "w", encoding="utf-8") as f:
                        json.dump({"timestamp": datetime.now().isoformat(), **out_obj}, f, ensure_ascii=False, indent=2)
                    log(f"written debug: {out_file}")
                    return out_obj
            # ---------- END: å³åˆ»å– ETA çš„è½»é‡å…œåº• ----------

            def scan_eta_in_page(p):
                # Returns ETA text or '' if not found
                try:
                    return p.evaluate("""
                        () => {
                          const norm = s => (s||'').replace(/\u00A0/g,' ').replace(/\s+/g,' ').trim();
                          const isDateish = s => {
                            if (!s) return false;
                            const t = norm(s).toUpperCase();
                            // very permissive: 2025-10-10, 10/10/2025, 10-OCT-2025, OCT-10-2025, 2025/10/10, etc.
                            return (
                              /^\d{4}[-\/.]\d{1,2}[-\/.]\d{1,2}/.test(t) ||
                              /^\d{1,2}[-\/.][A-Z]{3}[-\/.]\d{4}/.test(t) ||
                              /^[A-Z]{3}[-\/.]\d{1,2}[-\/.]\d{4}/.test(t)
                            );
                          };

                          // Fast exit: No Data.
                          const noData = Array.from(document.querySelectorAll('td')).some(td => norm(td.textContent) === 'No Data.');
                          if (noData) return '__NO_DATA__';

                          const labels = ['ESTIMATED ARRIVAL DATE','EST. ARRIVAL DATE','EST ARRIVAL DATE','ETA'];
                          const tds = Array.from(document.querySelectorAll('td'));
                          for (const td of tds) {
                            const txt = norm(td.textContent).toUpperCase();
                            if (labels.includes(txt)) {
                              const sib = td.nextElementSibling;
                              if (sib) {
                                const val = norm(sib.textContent);
                                if (txt === 'ETA') {
                                  if (isDateish(val)) return val;
                                } else {
                                  if (val) return val;
                                }
                              }
                            }
                          }
                          return '';
                        }
                    """)
                except Exception:
                    return ''

            def scan_eta_everywhere(page):
                # Scan root + frames
                val = scan_eta_in_page(page)
                if val: return val
                try:
                    for fr in page.frames:
                        try:
                            if fr is page.main_frame:
                                continue
                            _ = fr.evaluate_handle("() => document")  # touch frame
                            res = scan_eta_in_page(fr)
                            if res: return res
                        except Exception:
                            continue
                except Exception:
                    pass
                return ''


            # Poll up to ~30s, tolerant to JSF partial updates & iframes
            deadline = time.time() + 30
            eta_text = ''
            while time.time() < deadline:
                # quick no-data check & ETA scan
                val = scan_eta_everywhere(last_page)
                if val == '__NO_DATA__':
                    log("detected 'No Data.' during polling, exit as no result")
                    return {"status": "no_data", "error": "No Data."}
                if val:
                    eta_text = val
                    break
                # small sleep, also nudge network to 'idle'
                try:
                    last_page.wait_for_load_state('networkidle', timeout=2000)
                except Exception:
                    pass
                time.sleep(0.4)

            if not eta_text:
                # Save for debug and return
                try:
                    with open(os.path.join(debug_dir, "wanhai_final_page.html"), "w", encoding="utf-8") as f:
                        f.write(last_page.content())
                    log("saved wanhai_final_page.html for debug; ETA not found within polling window")
                except Exception:
                    pass
                # é¢å¤–ä¿å­˜æœ€ç»ˆé¡µé¢æˆªå›¾
                try:
                    last_page.screenshot(path=os.path.join(debug_dir, "wanhai_final_page.png"), full_page=True)
                except Exception:
                    pass
                return {"status": "timeout", "error": "ETA not found (no visible label or JSF fragment not rendered)"}

            before_norm = eta_text
            eta_text = normalize_date_text(eta_text)
            log(f"ETA normalize: before='{before_norm}' after='{eta_text}'")

            out_obj = {
                "status": "ok",
                "number": str(search_number),
                "result": eta_text,
                "clickedSearch": clicked_search_ok,
                "clickedMoreDetails": clicked_more_ok,
            }
            print(json.dumps(out_obj, ensure_ascii=False), flush=True)
            out_file = os.path.join(debug_dir, "wanhai_result.json")
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump({"timestamp": datetime.now().isoformat(), **out_obj}, f, ensure_ascii=False, indent=2)
            log(f"written debug: {out_file}")
            # é¢å¤–ä¿å­˜æœ€ç»ˆé¡µé¢æˆªå›¾
            try:
                last_page.screenshot(path=os.path.join(debug_dir, "wanhai_final_page.png"), full_page=True)
            except Exception:
                pass
            return out_obj
        except PlaywrightTimeoutError as e:
            try:
                page.screenshot(path=os.path.join(debug_dir, "wanhai_timeout.png"))
            except Exception:
                pass
            log(f"timeout exception after {elapsed_ms()}ms: {e}")
            return {"status": "timeout", "error": str(e)}
        except Exception as e:
            try:
                page.screenshot(path=os.path.join(debug_dir, "wanhai_error.png"))
            except Exception:
                pass
            log(f"unexpected exception after {elapsed_ms()}ms: {e}")
            return {"status": "error", "error": str(e)}
            
        finally:
            try:
                if context and not context.is_closed():
                    context.close()
                else:
                    log("context already closed or None, skip closing.")
            except Exception as e:
                log(f"ignore context close error: {e}")
  

def main():
    parser = argparse.ArgumentParser(description="WanHai tracking scraper using Playwright")
    parser.add_argument("--config", default=os.path.join("backend", "app", "config", "wanhai.json"),
                        help="path to json config")
    parser.add_argument("--number", help="override search_number from config", default=None)
    args = parser.parse_args()

    cfg_path = args.config
    if not os.path.isabs(cfg_path):
        # ç›¸å¯¹è·¯å¾„ä»¥é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ—¶å…¼å®¹
        root = os.path.dirname(os.path.dirname(__file__))
        cfg_path = os.path.abspath(os.path.join(root, os.path.relpath(cfg_path)))

    log(f"using config: {cfg_path}")
    config = load_config(cfg_path)
    if args.number:
        config["search_number"] = str(args.number)
        log(f"override search_number via --number: {config['search_number']}")
    ensure_dir(os.path.join(os.path.dirname(__file__), "app", "debug"))

    # æ‰§è¡Œï¼ˆscrape å†…éƒ¨å·²è´Ÿè´£æ‰“å°ä¸å†™è°ƒè¯•æ–‡ä»¶ï¼‰
    _ = scrape(config)


if __name__ == "__main__":
    sys.exit(main())


